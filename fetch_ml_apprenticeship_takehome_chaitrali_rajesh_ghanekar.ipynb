{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b0f563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.9/site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4899c409",
   "metadata": {},
   "source": [
    "## Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84219071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([2, 768])\n",
      "Obtained embeddings: tensor([[-0.0639, -0.4284, -0.0668,  ..., -0.1753, -0.1239,  0.3197],\n",
      "        [-0.1491, -0.4124, -0.0350,  ..., -0.1290,  0.2041,  0.0163]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class SentenceTransformer(nn.Module):\n",
    "    def __init__(self, pretrained_model_name='bert-base-uncased'):\n",
    "        super(SentenceTransformer, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.encoder = BertModel.from_pretrained(pretrained_model_name)\n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1) \n",
    "\n",
    "    def forward(self, sentences):\n",
    "        input_ids = self.tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        outputs = self.encoder(input_ids)\n",
    "        pooled_output = self.pooling(outputs.last_hidden_state.permute(0, 2, 1)).squeeze(-1)\n",
    "        return pooled_output\n",
    "\n",
    "sample_sentences = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
    "model = SentenceTransformer()\n",
    "embeddings = model(sample_sentences)\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Obtained embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151b5de0",
   "metadata": {},
   "source": [
    "I used the BertModel from the Hugging Face Transformers library as the transformer backbone.\n",
    "\n",
    "BertTokenizer is used to tokenize input sentences.\n",
    "\n",
    "I used an Adaptive Average Pooling layer to obtain fixed-length embeddings from the transformer output.\n",
    "\n",
    "The forward method takes a list of sentences, tokenizes them, passes through the transformer, and performs pooling to obtain embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b240e",
   "metadata": {},
   "source": [
    "## Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e98d1a",
   "metadata": {},
   "source": [
    "To expand the Sentence Transformer for multi-task learning, we need to modify the architecture to accommodate multiple tasks. Here, I have extended the model to handle two tasks: Sentence Classification (Task A) and Named Entity Recognition (Task B).\n",
    "\n",
    "Here are the changes made to the architecture:\n",
    "\n",
    "Task-Specific Heads: Adding task-specific classification heads on top of the transformer backbone for each task. These heads will be responsible for predicting task-specific outputs.\n",
    "\n",
    "Loss Function: For multi-task learning, I have defined a combined loss function that takes into account the losses from both tasks.\n",
    "\n",
    "Training Data: We need labeled data for both tasks. Each input will be associated with labels for both tasks.\n",
    "\n",
    "Fine-Tuning: During training, I fine-tuned the entire model, including the transformer backbone and the task-specific heads, using the combined loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7865e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A logits shape: torch.Size([2, 3])\n",
      "Task B logits shape: torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    def __init__(self, num_classes_task_a, num_classes_task_b, pretrained_model_name='bert-base-uncased'):\n",
    "        super(MultiTaskSentenceTransformer, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "        self.encoder = BertModel.from_pretrained(pretrained_model_name)\n",
    "        \n",
    "        # Task A classification head\n",
    "        self.classification_head_task_a = nn.Linear(self.encoder.config.hidden_size, num_classes_task_a)\n",
    "        \n",
    "        # Task B classification head\n",
    "        self.classification_head_task_b = nn.Linear(self.encoder.config.hidden_size, num_classes_task_b)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        input_ids = self.tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "        outputs = self.encoder(input_ids)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0] \n",
    "        \n",
    "        logits_task_a = self.classification_head_task_a(pooled_output)\n",
    "        \n",
    "        logits_task_b = self.classification_head_task_b(pooled_output)\n",
    "        \n",
    "        return logits_task_a, logits_task_b\n",
    "\n",
    "sample_sentences = [\"This is a sample sentence.\", \"Another example sentence.\"]\n",
    "model = MultiTaskSentenceTransformer(num_classes_task_a=3, num_classes_task_b=5)\n",
    "logits_task_a, logits_task_b = model(sample_sentences)\n",
    "print(\"Task A logits shape:\", logits_task_a.shape)\n",
    "print(\"Task B logits shape:\", logits_task_b.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147aee75",
   "metadata": {},
   "source": [
    "In this modified version:\n",
    "\n",
    "Two classification heads are added on top of the transformer backbone, one for each task (Task A and Task B).\n",
    "\n",
    "During forward pass, we obtain the hidden states from the transformer, then pass them through the task-specific classification heads to get logits for each task.\n",
    "\n",
    "The logits can be passed through appropriate activation functions and then used to compute the loss for each task during training.\n",
    "\n",
    "During training, we would need to combine the losses from both tasks using appropriate weighting or balancing techniques, depending on the importance of each task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bde45",
   "metadata": {},
   "source": [
    "## Task 3: Training Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08416b",
   "metadata": {},
   "source": [
    "Discuss the implications and advantages of each scenario and explain your rationale as to how the model should be trained given the following:\n",
    "\n",
    "If the entire network should be frozen.\n",
    "\n",
    "--> Implications: No parameters are updated during training, so the model essentially acts as a fixed feature extractor. It's beneficial if you have limited labeled data or if the pre-trained model captures relevant features for your tasks.\n",
    "\n",
    "Advantages: Training is faster, especially if you have limited computational resources. It reduces the risk of overfitting, especially if the labeled data for the tasks is limited.\n",
    "\n",
    "\n",
    "If only the transformer backbone should be frozen.\n",
    "\n",
    "--> Implications: Only the pre-trained weights of the transformer are kept frozen. Task-specific heads and additional layers are trainable. This scenario is beneficial when the pre-trained transformer captures general language understanding, but the downstream tasks require fine-tuning.\n",
    "\n",
    "Advantages: The model can adapt to task-specific features while still benefiting from the pre-trained language representation. It allows for task-specific feature extraction and avoids catastrophic forgetting of pre-trained representations.\n",
    "\n",
    "\n",
    "If only one of the task-specific heads (either for Task A or Task B) should be frozen.\n",
    "\n",
    "--> Implications: One of the task-specific heads is kept frozen while the other components are trainable. This approach can be beneficial if one task is more important or if you have limited labeled data for one task.\n",
    "\n",
    "Advantages: It allows focusing more training capacity on the task that needs more adaptation. It's useful when one task is relatively simpler or when you want to prioritize one task over the other.\n",
    "\n",
    "\n",
    "Consider a scenario where transfer learning can be beneficial. Explain how you would approach the transfer learning process, including:\n",
    "\n",
    "The choice of a pre-trained model.\n",
    "\n",
    "--> Choose a pre-trained model that has been pre-trained on a large corpus of text data, such as BERT, RoBERTa, or GPT. The choice depends on factors like model size, computational resources, and performance on similar tasks.\n",
    "\n",
    "The layers you would freeze/unfreeze.\n",
    "\n",
    "--> Freezing: Initially freeze all layers of the pre-trained model.\n",
    "\n",
    "Unfreezing: Gradually unfreeze layers starting from the top layers closer to the task-specific heads and fine-tune them along with the task-specific layers.\n",
    "\n",
    "The rationale behind these choices.\n",
    "\n",
    "--> Freezing the initial layers helps the model retain the general language understanding capabilities learned during pre-training.\n",
    "\n",
    "Gradually unfreezing allows the model to adapt to task-specific features while preventing catastrophic forgetting.\n",
    "Fine-tuning the top layers first is beneficial as they are closer to the task-specific heads and more likely to capture task-specific features.\n",
    "\n",
    "\n",
    "Training Process:\n",
    "\n",
    "Divide the training data into batches for each task.\n",
    "\n",
    "Use a suitable optimizer (e.g., Adam) with task-specific learning rates and weight decay.\n",
    "\n",
    "Monitor performance on both tasks using appropriate evaluation metrics.\n",
    "\n",
    "Employ techniques like early stopping and learning rate scheduling to prevent overfitting and improve convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548ca016",
   "metadata": {},
   "source": [
    "## Task 4: Layer-wise Learning Rate Implementation (BONUS) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930674d4",
   "metadata": {},
   "source": [
    "Implementing layer-wise learning rates involves assigning different learning rates to different layers of the neural network during training. This can be achieved by passing a list of learning rates to the optimizer, with each learning rate corresponding to a specific layer or group of layers in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a216f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rates = [\n",
    "    {\"params\": model.encoder.parameters(), \"lr\": 1e-5},  \n",
    "    {\"params\": model.classification_head_task_a.parameters(), \"lr\": 1e-4},  \n",
    "    {\"params\": model.classification_head_task_b.parameters(), \"lr\": 1e-4}  \n",
    "]\n",
    "\n",
    "optimizer = optim.Adam(learning_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1e81e",
   "metadata": {},
   "source": [
    "Rationale for specific learning rates:\n",
    "\n",
    "Transformer Backbone:\n",
    "\n",
    "The backbone captures general language representations. Since it's already pre-trained, we use a relatively lower learning rate to fine-tune it gradually. A small learning rate prevents drastic changes to the pre-trained weights.\n",
    "\n",
    "Task-Specific Heads:\n",
    "\n",
    "Task-specific heads require more adaptation to the specific tasks. Hence, we use a higher learning rate compared to the backbone. This allows faster updates to the task-specific parameters and facilitates task-specific feature learning.\n",
    "\n",
    "Potential benefits of using layer-wise learning rates:\n",
    "\n",
    "Faster Convergence:\n",
    "\n",
    "Layer-wise learning rates enable faster convergence by allowing different parts of the network to update at different rates. This can lead to faster training and better utilization of computational resources.\n",
    "\n",
    "Better Optimization:\n",
    "\n",
    "Assigning different learning rates to different layers can help avoid issues like vanishing gradients or exploding gradients, leading to more stable optimization.\n",
    "\n",
    "Task-Specific Adaptation:\n",
    "\n",
    "In a multi-task setting, layer-wise learning rates allow each task-specific head to adapt at its own pace, potentially improving performance on individual tasks.\n",
    "\n",
    "Improved Generalization:\n",
    "\n",
    "By controlling the learning rates for different layers, we can prevent overfitting on certain parts of the network and encourage better generalization across tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68801aab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
